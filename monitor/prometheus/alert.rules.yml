groups:
- name: UKWA metrics
  rules:
  - alert: w3act_db_backup_failed
    expr: absent(ukwa_database_backup_size_bytes{label="w3act"})
    for: 24h
    labels:
      severity: severe
    annotations:
      summary: "No recent database backup for W3ACT found on HDFS"
      description: "{{ $labels.instance }} of job {{ $labels.job }} failed to run."

  - alert: nominet_upload_not_on_hdfs
    expr: absent(trackdb_last_timestamp{label="nominet"}) or (time() - trackdb_last_timestamp{label="nominet"}) / (60*60*24) > 31
    for: 2h
    labels:
      severity: severe
    annotations:
      summary: "No recent upload of Nominet data was found on HDFS"
      description: "Job {{ $labels.job }} failed to run successfully."

  - alert: low_crawler_activity
    # The NPLD crawler is busier:
    expr: increase(heritrix3_crawl_job_uris_total{kind="finished", jobname!="frequent-bypm"}[4h]) < 10
    for: 10m
    labels:
      severity: severe
    annotations:
      description: Low crawl rate for crawl job {{ $labels.jobname }}.
      summary: The {{ $labels.jobname }} crawl job is crawling too slowly.

  - alert: low_crawler_activity_bypm
    # By permissions crawls are not running all day, but every day:
    expr: increase(heritrix3_crawl_job_uris_total{kind="finished",jobname="frequent-bypm"}[24h]) == 0
    for: 10m
    labels:
      severity: severe
    annotations:
      description: Low crawl rate for crawl job {{ $labels.jobname }}.
      summary: The {{ $labels.jobname }} crawl job is crawling too slowly.
      

  - alert: fc_crawl_is_slow
    expr: sum(rate(kafka_log_logendoffset{topic="fc.crawled"}[10m])) < 5
    for: 30m
    labels:
      severity: severe
    annotations:
      summary: "The frequent crawls are not running as fast as expected!"
      description: "The frequent crawls do not appear to be running as fast as it should be."

  - alert: trackdb_daily_refresh_has_not_run
    expr: absent(trackdb_refresh_timestamp) or (time() - trackdb_refresh_timestamp) / (60*60) > 24
    for: 1h
    labels:
      severity: severe
    annotations:
      summary: "TrackDB not updated in last 24 hours"
      description: "{{ $labels.instance }} {{ $labels.job }} refresh_timestamp_dt hasn't changed in over 24 hours"

  - alert: trackdb_no_new_warcs
    expr: absent(trackdb_last_timestamp_warcs) or (time() - trackdb_last_timestamp_warcs) / (60 * 60) > 24
    for: 8h
    labels:
      severity: severe
    annotations:
      summary: "No new WARCs on HDFS!"
      description: "According to TrackDB, no new WARCs have turned up on HDFS lately."
  
  - alert: trackdb_no_new_npld_crawl_logs
    expr: absent(trackdb_last_timestamp{label="npld.crawl-logs"}) or (time() - trackdb_last_timestamp{label="npld.crawl-logs"}) / (60 * 60) > 24
    for: 8h
    labels:
      severity: severe
    annotations:
      summary: "No new NPLD crawl logs on HDFS!"
      description: "According to TrackDB, no new NPLD crawl logs have turned up on HDFS lately."
  
  - alert: trackdb_no_new_warcs_cdxed
    expr: absent(trackdb_last_timestamp_cdx) or (time() - trackdb_last_timestamp_cdx) / (60 * 60) > 24
    for: 8h
    labels:
      severity: severe
    annotations:
      summary: "No WARCs have been CDX-indexed recently!"
      description: "According to TrackDB, no new WARCs have been CDX-indexed lately."
  
  - alert: cdx_oa_no_new_mementos
    expr: absent(cdx_oa_wayback_last_timestamp) or (time() - cdx_oa_wayback_last_timestamp) / (60 * 60) > 48
    for: 2h
    labels:
      severity: severe
    annotations:
      summary: "No new Mementos available via OA Wayback!"
      description: "According to the PyWB CDX API, no new content has turned up for a site we expect to visit every day."

  - alert: trackdb_no_recent_rr_logs
    expr: absent(trackdb_numFound_rr_logs) or trackdb_numFound_rr_logs != 10
    for: 12h
    labels:
      severity: severe
    annotations:
      summary: "Missing Reading Room log files!"
      description: "According to TrackDB, there are not the expected number (10) of up-to-date log files on HDFS."

  - alert: airflow_dags_failing
    expr: airflow_dag_last_status{status='failed'} > 0
    for: 10m
    labels:
      severity: severe
    annotations:
      summary: "Airflow reports {{ $labels.dag_id }} is failing"
      description: "The Airflow service at {{ $labels.instance }} is reporting that the most recent run of DAG workflow {{ $labels.dag_id }} has failed."

  - alert: gluster_fc_filling_up
    expr: delta(ukwa_files_count{fs="gluster", job="warc_tidy", kind="warcs"}[12h] ) > 0
    for: 12h
    labels:
      severity: severe
    annotations:
      summary: "The FC output volume is filling up with WARCs"
      description: "The number of WARCs on the FC Gluster volume appears to be increasing: check move-to-hdfs is working as expected."

  - alert: tidy-logs_no_new_crawl_logs
    expr: delta(ukwa_crawler_log_size_bytes{log='crawl.log'}[1h]) == 0 or absent(ukwa_crawler_log_size_bytes{log='crawl.log'})
    for: 1h
    labels:
      severity: severe
    annotations:
      summary: "No new crawl logs from tidy-logs"
      description: "{{ $labels.instance }} of job {{ $labels.job }} failed to run."


- name: Generic metrics
  rules:

  # Alert for any instance that is unreachable for >5 minutes.
  - alert: service_down
    expr: up == 0
    for: 5m
    labels:
      severity: severe
    annotations:
      summary: "Service {{ $labels.instance }} down"
      description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes."

  - alert: host_cpu_rate
    expr: 100 - 100*avg(irate(node_cpu_seconds_total{mode='idle'}[10m])) > 80
    for: 10m
    labels:
      severity: severe
    annotations:
      summary: "CPU rate on {{ $labels.instance }} > 80%"
      description: "CPU rate on {{ $labels.instance }} > 80% for more than 10 minutes"

  - alert: host_free_disk_space
    expr: (node_filesystem_avail_bytes{fstype=~"ext.|xfs|zfs",instance!="jisc03:9100"}/node_filesystem_size_bytes{fstype=~"ext.|xfs|zfs",instance!="jisc03:9100"}) * 100 < 15
    for: 5m
    labels:
      severity: severe
    annotations:
      summary: "Disk space free < 15% on {{ $labels.instance }}"
      description: "Disk space free on {{ $labels.instance }} has been less that 15% for more than 5 minutes"

  - alert: host_free_disk_space_jisc03
    expr: (node_filesystem_avail_bytes{fstype=~"ext.|xfs|zfs",instance="jisc03:9100"}/node_filesystem_size_bytes{fstype=~"ext.|xfs|zfs",instance="jisc03:9100"}) * 100 < 5
    for: 5m
    labels:
      severity: severe
    annotations:
      summary: "Disk space free < 5% on {{ $labels.instance }}"
      description: "Disk space free on {{ $labels.instance }} has been less that 5% for more than 5 minutes"

  - alert: predict_host_disk_space
    expr: min(predict_linear(node_filesystem_free{mountpoint=~"/|/data"}[48h], 2*7*24*3600)) by (instance, mountpoint) < 0
    for: 1h
    labels:
      severity: severe
    annotations:
      summary: "Disk space running out on {{ $labels.instance }} at {{ $labels.mountpoint }}"
      description: "Based on recent sampling, the disk is likely to will fill on volume {{ $labels.mountpoint }} within the next two weeks, for instance: {{ $labels.instance }}."

  - alert: cpu_running_too_hot
    expr: max(node_hwmon_temp_celsius) by (instance) > 70
    for: 30m
    labels:
      severity: severe
    annotations:
      summary: "CPU running too hot?"
      description: "The CPU on {{ $labels.instance }} is running hot (>70C for 30mins)."
  
  - alert: ssl_certs_nearing_expiration
    expr: (probe_ssl_earliest_cert_expiry - time())/(60*60*24) < 30
    for: 1h
    labels:
      severity: severe
    annotations:
      summary: "SSL certificate for {{ $labels.instance }} will expire soon!"
      description: "The SSL certificate for {{ $labels.instance }} (part of {{ $labels.job }}) will expire in less than 30 days."


